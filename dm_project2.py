# -*- coding: utf-8 -*-
"""DM_Project2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gzpe3JSBjgjjKjTM3hehEGuGsCiX7PFu
"""

#@Title: Project 2 Data Mining
#@Purpose: Implement Bag of words with different classifiers and implement Keras
#@Authors: Nickolas Gadomski, Colby Nicoletti, Carlos Martinez
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.layers import Embedding
from keras.layers import Conv1D, GlobalMaxPooling1D
from keras.preprocessing.text import Tokenizer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, confusion_matrix

data = pd.read_csv("data_incl_demographics filtered_SMALL.csv")

df = pd.DataFrame(data, columns = ['worry', 'text_long'])
print(df)

#Initializes worry column with matching string 'low'
worry_low = len(df[df['worry'].str.match('low')])
print('Total number of low worries:\n', worry_low)

#Initializes worry column with matching string 'med'
worry_med = len(df[df['worry'].str.match('med')])
print('Total number of medium worries:\n', worry_med)

#Initializes worry column with matching string 'high'
worry_high = len(df[df['worry'].str.match('high')])
print('Total number of high worries:\n', worry_high,'\n')

#Total number of worries/records
worry_total = worry_low + worry_med + worry_high
print('The total number of worries or records is:\n', worry_total, '\n')

percent_low = (worry_low / worry_total) * 100
print('The percentage of low worries is:\n', percent_low)

percent_med = (worry_med / worry_total) * 100
print('The percentage of medium worries is:\n', percent_med)

percent_high = (worry_high / worry_total) * 100 
print('The percentage of high worries is:\n', percent_high, '\n')

#print(df.shape)
print("This is the shape e.g. unique words in an array")
#This is done both with/without stopwords to get the amount of unique words
unsplitVectorizer = CountVectorizer(stop_words='english')
unsplit = unsplitVectorizer.fit_transform(df['text_long']).toarray()
print(unsplit)
print('Total of records and words:\n', unsplit.shape)


#Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df['text_long'], 
                                                    df['worry'], 
                                                    test_size=0.2,
                                                    random_state=42)

#Count and Tfdif vectorizer both used to see which is more accurate
print('\nCount Vectorizer:')
vectorizer = CountVectorizer()
X_train_count = vectorizer.fit_transform(X_train).toarray()
X_test_count = vectorizer.fit_transform(X_test).toarray()
print(X_train_count.shape)
print(X_test_count.shape, '\n')
#print(X_test)
#print(X_train)

#Frequency Vectorizer
print('\nTfidf Vectorizer:')
Tfidf_vectorizer = TfidfVectorizer(stop_words="english")
X_train_tfidf = Tfidf_vectorizer.fit_transform(X_train).toarray()
X_test_tfidf = Tfidf_vectorizer.fit_transform(X_test).toarray()
print(X_train_tfidf.shape)
print(X_test_tfidf.shape)

#KNN implementation
bowVect = Tfidf_vectorizer.fit(X_train)

bowTrain = bowVect.transform(X_train)
bowTest = bowVect.transform(X_test)

knn = KNeighborsClassifier(n_neighbors = 9)
knn.fit(bowTrain, y_train )
KNN_predicted = knn.predict(bowTest)

from sklearn.metrics import f1_score
from sklearn.metrics import average_precision_score


#Decision Tree Implementation
from sklearn import tree
clf = tree.DecisionTreeClassifier()
clf = clf.fit(bowTrain, y_train)
DT_predicted = clf.predict(bowTest)


#Naive Bayes implementation
clf = MultinomialNB()
clf.fit(bowTrain, y_train)
NB_predicted = clf.predict(bowTest)

print("Confusion matrix Decision Tree")
print(confusion_matrix(y_test,DT_predicted))
print("Classification report for Decision Tree")
print(classification_report(y_test,DT_predicted))

print("Confusion matrix k-NN")
print(confusion_matrix(y_test,KNN_predicted))
print("Classification report for k-NN")
print(classification_report(y_test,KNN_predicted))

print("Confusion matrix Naive Bayes")
print(confusion_matrix(y_test,NB_predicted))
print("Classification report for Naive Bayes")
print(classification_report(y_test,NB_predicted))

from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.layers import Embedding
from keras.layers import Conv1D, GlobalMaxPooling1D
from keras.preprocessing.text import Tokenizer
import pandas as pd
from sklearn.model_selection import train_test_split

# set parameters for keras
max_features = 20000
maxlen = 50
batch_size = 50
embedding_dims = 100
filters = 250
kernel_size = 3
hidden_dims = 10
epochs = 15

#Read CSV data
print('Loading data...')
df = pd.read_csv("data_incl_demographics filtered_SMALL.csv")
df.worry = pd.Categorical(df.worry)
df['code'] = df.worry.cat.codes
docs_df = df['text_long']
scores_df = df['code']
print(df['code'])

# how many in each class
scores = range(0, 3)
for sc in scores:
    print('Num w/ score of {}: {}'.format(sc, len(df[df['code'] == sc])))

# train test split
x_train, x_test, y_train, y_test = train_test_split( docs_df, 
                                                    scores_df, 
                                                    test_size=0.2, 
                                                    random_state=42)

# tokenize based on train dataset
tokenizer = Tokenizer(max_features)
tokenizer.fit_on_texts(x_train)

x_train = tokenizer.texts_to_sequences(x_train)
x_test = tokenizer.texts_to_sequences(x_test)
vocab_size = len(tokenizer.word_index) + 1 

# print some example records and size
print(len(x_train), 'train sequences')
print(len(x_test), 'test sequences')
#print(x_train[2])
#print(docs_df[2])
#print(scores_df[2])
#print(vocab_size)

print('Pad sequences (samples x length)')
x_train = sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = sequence.pad_sequences(x_test, maxlen=maxlen)
print('x_train shape:', x_train.shape)
print('x_test shape:', x_test.shape)

print('Build model...')
model = Sequential()

# we start off with an efficient embedding layer which maps
# our vocab indices into embedding_dims dimensions
model.add(Embedding(max_features,
                    embedding_dims,
                    input_length=maxlen))
model.add(Dropout(0.2))

# we add a Convolution1D, which will learn filters
# word group filters of size filter_length
model.add(Conv1D(filters,
                 kernel_size,
                 padding='valid',
                 activation='relu',
                 strides=1))
# we use max pooling
model.add(GlobalMaxPooling1D())

# We add a vanilla hidden layer
model.add(Dense(hidden_dims))
model.add(Dropout(0.2))
model.add(Activation('relu'))

# We project onto a single unit output layer, and squash it with a sigmoid
model.add(Dense(1))
model.add(Activation('sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          validation_data=(x_test, y_test))

#######PLotting Keras#######################
from keras.models import Sequential
from keras import layers
import matplotlib.pyplot as plt

input_dim = x_train.shape[1]  # Number of features

model = Sequential()
model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', 
              optimizer='adam', 
              metrics=['accuracy'])
model.summary()

history = model.fit(x_train, y_train,
                     epochs=15,
                     verbose=False,
                     validation_data=(x_test, y_test),
                     batch_size=50)

loss, accuracy = model.evaluate(x_train, y_train, verbose=False)
print("Training Accuracy: {:.4f}".format(accuracy))
loss, accuracy = model.evaluate(x_test, y_test, verbose=False)
print("Testing Accuracy:  {:.4f}".format(accuracy))


plt.style.use('ggplot')

def plot_history(history):
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    x = range(1, len(acc) + 1)

    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(x, acc, 'b', label='Training accuracy')
    plt.plot(x, val_acc, 'r', label='Validation accuracy')
    plt.title('Training and validation accuracy')
    plt.legend()
    plt.subplot(1, 2, 2)
    plt.plot(x, loss, 'b', label='Training loss')
    plt.plot(x, val_loss, 'r', label='Validation loss')
    plt.title('Training and validation loss')
    plt.legend()

plot_history(history)

import pandas as pd
import tensorflow as tf
df = pd.read_csv("data_incl_demographics filtered_SMALL.csv")

df.dtypes

df.head()